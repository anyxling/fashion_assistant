{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEN-Denim-id_00000080-01_7_additional.jpg</td>\n",
       "      <td>The lower clothing is of long length. The fabr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEN-Denim-id_00000089-01_7_additional.jpg</td>\n",
       "      <td>His tank top has sleeves cut off, cotton fabri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEN-Denim-id_00000089-02_7_additional.jpg</td>\n",
       "      <td>His sweater has long sleeves, cotton fabric an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEN-Denim-id_00000089-03_7_additional.jpg</td>\n",
       "      <td>His shirt has short sleeves, cotton fabric and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEN-Denim-id_00000089-04_7_additional.jpg</td>\n",
       "      <td>The sweater the person wears has long sleeves,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42539</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979-04_4_full.jpg</td>\n",
       "      <td>The lady wears a tank tank shirt with pure col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42540</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979-04_7_additional.jpg</td>\n",
       "      <td>The person wears a sleeveless tank shirt with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42541</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007981-03_1_front.jpg</td>\n",
       "      <td>This woman wears a sleeveless tank top with ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42542</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007981-03_3_back.jpg</td>\n",
       "      <td>The tank top the lady wears has sleeves cut of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42543</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007981-03_7_additional.jpg</td>\n",
       "      <td>The lady wears a tank tank shirt with complica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42544 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  \\\n",
       "0             MEN-Denim-id_00000080-01_7_additional.jpg   \n",
       "1             MEN-Denim-id_00000089-01_7_additional.jpg   \n",
       "2             MEN-Denim-id_00000089-02_7_additional.jpg   \n",
       "3             MEN-Denim-id_00000089-03_7_additional.jpg   \n",
       "4             MEN-Denim-id_00000089-04_7_additional.jpg   \n",
       "...                                                 ...   \n",
       "42539        WOMEN-Tees_Tanks-id_00007979-04_4_full.jpg   \n",
       "42540  WOMEN-Tees_Tanks-id_00007979-04_7_additional.jpg   \n",
       "42541       WOMEN-Tees_Tanks-id_00007981-03_1_front.jpg   \n",
       "42542        WOMEN-Tees_Tanks-id_00007981-03_3_back.jpg   \n",
       "42543  WOMEN-Tees_Tanks-id_00007981-03_7_additional.jpg   \n",
       "\n",
       "                                                    text  \n",
       "0      The lower clothing is of long length. The fabr...  \n",
       "1      His tank top has sleeves cut off, cotton fabri...  \n",
       "2      His sweater has long sleeves, cotton fabric an...  \n",
       "3      His shirt has short sleeves, cotton fabric and...  \n",
       "4      The sweater the person wears has long sleeves,...  \n",
       "...                                                  ...  \n",
       "42539  The lady wears a tank tank shirt with pure col...  \n",
       "42540  The person wears a sleeveless tank shirt with ...  \n",
       "42541  This woman wears a sleeveless tank top with ot...  \n",
       "42542  The tank top the lady wears has sleeves cut of...  \n",
       "42543  The lady wears a tank tank shirt with complica...  \n",
       "\n",
       "[42544 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = pd.read_json('captions.json', orient='index')\n",
    "captions.reset_index(inplace=True)\n",
    "captions.columns = ['image_path', 'text']\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionOutfitDataset(Dataset):\n",
    "    def __init__(self, data, preprocess, tokenizer):\n",
    "        self.data = data\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.data.iloc[index, 0]\n",
    "        text = self.data.iloc[index, 1]\n",
    "\n",
    "        image = self.preprocess(Image.open(f'images/{image_path}').convert(\"RGB\"))\n",
    "        tokens = self.tokenizer([text])[0]\n",
    "\n",
    "        return image, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion2b_s34b_b79k'\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "dataset = FashionOutfitDataset(captions, preprocess, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "def train_clip(model, dataloader, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, texts in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "\n",
    "            # normalize\n",
    "            image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "            logits_per_image = image_features @ text_features.T\n",
    "            logits_per_text = text_features @ image_features.T\n",
    "\n",
    "            labels = torch.arange(len(images), device=device)\n",
    "\n",
    "            loss = (loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)) / 2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1330 [01:50<5:03:52, 13.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mtrain_clip\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m (loss_fn(logits_per_image, labels) \u001b[38;5;241m+\u001b[39m loss_fn(logits_per_text, labels)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/python-general/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python-general/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_clip(model, dataloader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
